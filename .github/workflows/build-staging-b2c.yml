name: Staging B2C Branch
on:
  pull_request_target:
    types:
      - closed
    branches:
      - 'staging'
  workflow_dispatch:
    inputs:
      choice:
        type: choice
        description: Make a choice
        required: true
        default: restart
        options:
          - deploy
          - restart

env:
  AWS_Account: 542649758539
  Region: ap-south-1
  ClusterName: Staging-Magnet
  Env: staging
  NameSpace: staging
  ValuesFile: values.staging.yaml
  SONAR_HOST_URL: https://sonar-eks.staging.onecode.in
  SONAR_TOKEN: ${{ secrets.SONAR_TOKEN_STAGING }}
  SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK_URL_STAGING }}
  SLACK_CHANNEL: '#gitactions-staging'
  ECR_REGISTRY: 542649758539.dkr.ecr.ap-south-1.amazonaws.com
  ECR_REPOSITORY: wee-api-staging
  AppName: wee-api


jobs:
  Build-Deploy-Service:
    if: ( github.event_name == 'workflow_dispatch' && github.event.inputs.choice  == 'deploy' ) || ( github.event_name == 'pull_request_target' && github.event.action == 'closed' && github.event.pull_request.merged == true )
    runs-on: ubuntu-latest
    timeout-minutes: 15

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          role-to-assume: arn:aws:iam::542649758539:role/github-actions-role
          role-session-name: githubactions
          aws-region: ${{ env.Region }}


      - name: Checkout Git Repository code
        uses: actions/checkout@v3

      - name: Set up Java env
        uses: actions/setup-java@v3
        with:
          java-version: 11
          distribution: 'adopt'
          cache: gradle

      - name: Setup Gradle
        uses: gradle/gradle-build-action@v2
        with:
          gradle-version: 8.1.1

      - name: Execute Gradle wrapper
        run: gradle wrapper

      - name: Execute Gradle build
        run:  ./gradlew build -x test

      - name: Login to ECR
        uses: docker/login-action@v2
        with:
          registry: ${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2

      - name: Set up Docker Buildx
        id: buildx
        uses: docker/setup-buildx-action@v2

      - name: Getting Docker tag from github SHA value
        run: echo "IMAGE_TAG=`echo ${{ github.sha }}| cut -c1-8`" >> $GITHUB_ENV

      - name: Deleting old docker imager on manual deploy action
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.choice  == 'deploy'
        run: |
          OLD_COMMIT_ID=$(aws ecr describe-images --repository-name ${{ env.ECR_REPOSITORY }} --query 'sort_by(imageDetails,& imagePushedAt)[-1].imageTags[0]' --output text)
          echo "$OLD_COMMIT_ID"
          if [[ "$OLD_COMMIT_ID" == ${{ env.IMAGE_TAG }} ]]; then
                aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --image-ids imageTag=${{ env.IMAGE_TAG }}
                echo "Deleting docker image"
          else
                echo "Its New Deployment... Proceeding with next step"
          fi

      - name: Docker Images Build and push to ECR
        id: docker_build
        uses: docker/build-push-action@v3
        with:
          context: ./
          #           # Dockerfile file location
          file: ./Dockerfile
          #           # Supported image architectures, since I need an aarch64 image, so I added arm64 here

          push: true
          tags: ${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}

      - name: Create kube config
        run: |
          aws eks update-kubeconfig --region "${{ env.Region }}" --name "${{ env.ClusterName }}"

      - name: Deploy on EKS
        id: eks
        run: |
          helm upgrade --install "${{ env.AppName }}" ./deployment-files-B2C/helm/  -f ./deployment-files-B2C/helm/"${{ env.ValuesFile }}" -n "${{ env.NameSpace }}" --create-namespace   --set image.repository=${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }} --set image.tag="${{ env.IMAGE_TAG }}" --set region="${{ env.Region }}" --debug  

      - name: Deployment Failed
        if: failure()
        run: |
          echo "Kubernetes deployment Failed. Deleting Docker Image"
          aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --image-ids imageTag=${{ env.IMAGE_TAG }}
          
          # Set the GitHub Actions job as failed
          exit 1


      - name: Check Pod Deployment Status
        id: pod-status
        timeout-minutes: 5
        run: |
          
          TIMEOUT=100
          echo "Default Timeout 5 mins"
          POD_NAME=$(kubectl get pods -n "${{ env.NameSpace }}" --sort-by='.metadata.creationTimestamp' --selector=appname="${{ env.AppName }}" | awk 'END{print $1}')
          
          for i in $(seq 1 $TIMEOUT); do
          
             pod_status=$(kubectl get pod $POD_NAME -n "${{ env.NameSpace }}" -o jsonpath='{.status.phase}')
             ready_status=$(kubectl get pod -n "${{ env.NameSpace }}" $POD_NAME -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
          
             if [[ "$pod_status" == "Running"  &&  "$ready_status" == "True" ]]; then
          
               echo "::set-output name=pod_status::Running"
               echo "::set-output name=ready_status::True"
          
               echo -e "\e[1mPod is Running.\e[0m"
               kubectl get pods -n "${{ env.NameSpace }}" | grep -i "${{ env.AppName }}"
               break
             fi
          
             i=i+1
             echo -e "\e[1mPod Status....\e[0m"
             kubectl get pods -n "${{ env.NameSpace }}" | grep -i "${{ env.AppName }}"
          
             sleep 1
          
          done
          rm ~/.kube/config

      - name: Deployment | Job Failed. Rolling back
        if: (failure() || steps.pod-status.outcome == 'Failure' || steps.pod-status.timed_out) || ( steps.pod-status.outputs.pod_status != 'Running' && steps.pod-status.outputs.ready_status != 'True')
        run: |
          aws eks update-kubeconfig --region "${{ env.Region }}" --name "${{ env.ClusterName }}" 
          POD_NAME=$(kubectl get pods -n "${{ env.NameSpace }}" --sort-by='.metadata.creationTimestamp' --selector=appname="${{ env.AppName }}" | awk 'END{print $1}')
          echo -e "\e[1mLogs of Pod.\e[0m"
          kubectl logs $POD_NAME -n "${{ env.NameSpace }}"
          
          
          Image_tag=$(kubectl get deployment "${{ env.AppName }}" -n "${{ env.NameSpace }}" -o jsonpath='{.spec.template.spec.containers[0].image}' | cut -d ":" -f2)
          if [[ "$Image_tag" != "${{ env.IMAGE_TAG }}" ]]; then
              echo -e "\e[1mJob Failure Deleting Docker Image\e[0m"
              aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --image-ids imageTag=${{ env.IMAGE_TAG }}
          
          elif [[ steps.docker_build.outcome == 'Success' && steps.pod-status.outcome == 'Failure' || steps.pod-status.timed_out ]]; then
               echo -e "\e[1m Deleting deployed Docker Image\e[0m"
               aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --image-ids imageTag=${{ env.IMAGE_TAG }}
               echo -e "\e[1mPod is not in the Running state. Initiating rollback....\e[0m"
               OLD_COMMIT_ID=$(aws ecr describe-images --repository-name ${{ env.ECR_REPOSITORY }} --query 'sort_by(imageDetails,& imagePushedAt)[-1].imageTags[0]' --output text)
               echo "$OLD_COMMIT_ID"
               helm upgrade --install "${{ env.AppName }}" ./deployment-files-B2C/helm/  -f ./deployment-files-B2C/helm/"${{ env.ValuesFile }}" -n "${{ env.NameSpace }}" --create-namespace   --set image.repository=${{ env.ECR_REGISTRY }}/${{ env.ECR_REPOSITORY }} --set image.tag="$OLD_COMMIT_ID" --set region="${{ env.Region }}" --debug
               #helm rollback -n "${{ env.NameSpace }}" "${{ env.AppName }}" $(helm list -n "${{ env.NameSpace }}" --filter "${{ env.AppName }}" --output json | jq '.[0].revision | tonumber - 1')

          else
             echo "Deployment not rolled back"
          fi 
          rm ~/.kube/config
          
          # Set the GitHub Actions job as failed
          exit 1

  #      - name: Slack Notification
  #        uses: rtCamp/action-slack-notify@v2
  #        if: always()
  #        env:
  #          SLACK_CHANNEL: ${{ env.SLACK_CHANNEL }}
  #          SLACK_COLOR: ${{ job.status }} # or a specific color like 'good' or '#ff00ff'
  #          SLACK_MESSAGE: "${{ env.AppName }} is ${{ job.status }}"
  #          SLACK_TITLE: 'Build Status'
  #          SLACK_WEBHOOK: ${{ env.SLACK_WEBHOOK }}


  Restart-Pod:
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.choice  == 'restart'
    runs-on: ubuntu-latest
    timeout-minutes: 10
    permissions:
      id-token: write
      contents: read

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          role-to-assume: arn:aws:iam::542649758539:role/github-actions-role
          role-session-name: githubactions
          aws-region: ${{ env.Region }}

      - name: Restart Pod on EKS
        run: |
          aws eks update-kubeconfig --region "${{ env.Region }}" --name "${{ env.ClusterName }}"
          
          kubectl rollout restart deployment/"${{ env.AppName }}" -n "${{ env.NameSpace }}" 
          
          echo -e "\e[1mRestart Complete.\e[0m"

      - name: Check Pod Ready Status
        id: pod-ready
        timeout-minutes: 5
        run: |
          
          TIMEOUT=100
          echo "Default Timeout 5 mins"
          POD_NAME=$(kubectl get pods -n "${{ env.NameSpace }}" --sort-by='.metadata.creationTimestamp' --selector=appname="${{ env.AppName }}" | awk 'END{print $1}')
          
          for i in $(seq 1 $TIMEOUT); do
          
             pod_status=$(kubectl get pod $POD_NAME -n "${{ env.NameSpace }}" -o jsonpath='{.status.phase}')
             ready_status=$(kubectl get pod -n "${{ env.NameSpace }}" $POD_NAME -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
          
             if [[ "$pod_status" == "Running"  &&  "$ready_status" == "True" ]]; then
          
               echo "::set-output name=pod_restart_status::Running"
               echo "::set-output name=restart_ready_status::True"
          
               echo -e "\e[1mCogratulations.\e[0m"
               echo -e "\e[1mPod is Running\e[0m"
          
               kubectl get pods -n "${{ env.NameSpace }}" | grep -i "${{ env.AppName }}"
               break
             fi
          
             i=i+1
             echo -e "\e[1mPod Status....\e[0m"
             kubectl get pods -n "${{ env.NameSpace }}" | grep -i "${{ env.AppName }}"
          
             sleep 1
          
          done
          
          rm ~/.kube/config



      - name: Pod Restart Failure Status
        if: (failure() || steps.pod-ready.outcome == 'failure') || (steps.pod-ready.outputs.pod_restart_status != 'Running' && steps.pod-ready.outputs.restart_ready_status != 'True')
        run: |
          aws eks update-kubeconfig --region "${{ env.Region }}" --name "${{ env.ClusterName }}"

          POD_NAME=$(kubectl get pods -n "${{ env.NameSpace }}" --sort-by='.metadata.creationTimestamp' --selector=appname="${{ env.AppName }}" | awk 'END{print $1}')
          echo -e "\e[1mLogs of Pod.\e[0m"
          kubectl logs $POD_NAME -n "${{ env.NameSpace }}"
          
          echo -e "\e[1mPod is not in the Running state.\e[0m"
          echo -e "\e[1mRestart Failed.\e[0m"
          echo -e "\e[1mKindly Deploy the Service Again ...\e[0m"
          
          
          
          rm ~/.kube/config

      - name: Slack Notification
        uses: rtCamp/action-slack-notify@v2
        if: always()
        env:
          SLACK_CHANNEL: ${{ env.SLACK_CHANNEL }}
          SLACK_COLOR: ${{ job.status }} # or a specific color like 'good' or '#ff00ff'
          SLACK_MESSAGE: "${{ env.AppName }} is ${{ job.status }}"
          SLACK_TITLE: 'Restart Status'
          SLACK_WEBHOOK: ${{ env.SLACK_WEBHOOK }}